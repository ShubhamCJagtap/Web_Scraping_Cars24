{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ee3d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Cars24_web_scrape.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Cars24_web_scrape.py\n",
    "\"\"\"\n",
    "    Developed By : Shubham Jagtap\n",
    "    linkedIn : https://www.linkedin.com/in/shubham-jagtap-scj4497/\n",
    "    \n",
    "    Please install Beautiful Soup and Selenium 4\n",
    "    \n",
    "    How to use program:\n",
    "    1. Run the File\n",
    "    2. Provide the city name you want to scrape from\n",
    "    3. Program will automatically scrape the data and it will store it in CSV format\n",
    "    \n",
    "    \n",
    "    Enjoy Cars24 scraping\n",
    "\n",
    "\"\"\"\n",
    "#------------------------------------ IMPORT LIBRARIES\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def start_browser():\n",
    "    \"\"\"\n",
    "    Start the Edge web browser \n",
    "    \"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    print('Browser Started...')\n",
    "    return driver\n",
    "#--------------------------------------------------------------------------------\n",
    "def get_url():\n",
    "    print('Searching cars24.com...')\n",
    "    template = \"https://www.cars24.com/buy-used-car?sort=P&storeCityId=2&pinId=110001\"\n",
    "    print(template)\n",
    "    print('....')\n",
    "    return template\n",
    "#--------------------------------------------------------------------------------\n",
    "def get_city(driver):\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    cities = []\n",
    "    try:\n",
    "        cities = (soup.find('ul',{'class':'popularCities'})).children\n",
    "        print('Great Mega City present')\n",
    "    except:\n",
    "        print('I think your mega city is not present')\n",
    "        print('trying one more time')\n",
    "        try:\n",
    "            cities = (soup.find('ul',{'class':'otherCities'})).children\n",
    "            print('Hurray!!! City Present')\n",
    "        except:\n",
    "            print('Oops!! city is not present')\n",
    "            print('Try any big city near you')\n",
    "    return soup,cities\n",
    "#--------------------------------------------------------------------------------\n",
    "def click(driver):\n",
    "    try:\n",
    "        soup,cities = get_city(driver)\n",
    "        driver.find_element(By.CSS_SELECTOR,\"img[data-title='{}']\".format(next(cities).get('data-title'))).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        soup,cities = get_city(driver)\n",
    "        print('first pass failed!!! trying second pass')\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,\"//li[text()='{}']\".format(next(cities).text)).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        soup,cities = get_city(driver)\n",
    "        print('second pass failed!!! trying second pass')\n",
    "    try:\n",
    "        driver.find_element(By.LINK_TEXT,\"{}\".format(next(cities).text)).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        print('Oops..... no city is present to click')\n",
    "#--------------------------------------------------------------------------------\n",
    "def print_title(soup):\n",
    "    title = \"As per cars24 details : \"+str(soup.find('div',{'class':'js-content'}).find(\"h1\").text)\n",
    "    print(title)\n",
    "    print('NOTE: total cars on website can be different')\n",
    "#--------------------------------------------------------------------------------\n",
    "def save_file(cars):\n",
    "    all_cars = []\n",
    "    print(\"observed entries: \",len(cars))\n",
    "    for i,j in enumerate(tqdm(cars,desc = \"Collecing data :...\")):\n",
    "        try:\n",
    "            title = cars[i].find('h2',{'class':'_3FpCg'}).text\n",
    "            model = (\" \").join(cars[i].find('p',{'class':'cvakB'}).text.split(' ')[:-1])\n",
    "            gear_type = cars[i].find('p',{'class':'cvakB'}).text.split(' ')[-1]\n",
    "            km = cars[i].find_all('li')[0].text\n",
    "            owner = cars[i].find_all('li')[1].text\n",
    "            fuel = cars[i].find_all('li')[2].text\n",
    "            state_passing = cars[i].find_all('li')[3].text\n",
    "            price = cars[i].find('div',{'class':'_7udZZ'}).text.replace(\"₹\",\"\")\n",
    "            all_cars.append((title,model,gear_type,km,owner,fuel,state_passing,price))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"total cars collected \",len(all_cars))\n",
    "    print(\"Saving in CSV file\")\n",
    "    filename='cars data-'+str(city_name)+str('.csv')\n",
    "    with open(filename,'w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Car_Name','version','Gear Type','Running_kn','Owner','Fuel Type','Passing State','Price'])\n",
    "        writer.writerows(all_cars)\n",
    "#--------------------------------------------------------------------------------\n",
    "# MAIN CODE\n",
    "\n",
    "city_name = str(input('Please share city name: '))\n",
    "driver = start_browser()\n",
    "driver.get(get_url())\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[1]/div[2]/p\").click()\n",
    "a = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div[1]/div/div[2]/div/input\")\n",
    "try:\n",
    "    driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div[1]/div/div[2]/img\").click()\n",
    "except:\n",
    "    print('Search box is clear.....')\n",
    "a.send_keys(city_name)\n",
    "click(driver)\n",
    "time.sleep(5)\n",
    "print_title(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "\n",
    "l = 0\n",
    "while(True):\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    cars = soup.find_all('div',{'class':'col-4'})\n",
    "    if l == len(cars):\n",
    "        print(\"reached to end\")\n",
    "        break\n",
    "    else:\n",
    "        l = len(cars)\n",
    "        print('scrolling Down')\n",
    "        print('current_count :'+str(len(cars))+\" \"+str(l))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(8)\n",
    "\n",
    "save_file(cars)\n",
    "driver.quit()\n",
    "print('Browser Closed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd57b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_browser():\n",
    "    \"\"\"\n",
    "    Start the Edge web browser \n",
    "    \"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    print('Browser Started...')\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0f83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url():\n",
    "    print('Searching cars24.com...')\n",
    "    template = \"https://www.cars24.com/buy-used-car?sort=P&storeCityId=2&pinId=110001\"\n",
    "    print(template)\n",
    "    print('....')\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee142bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(driver):\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    cities = []\n",
    "    try:\n",
    "        cities = (soup.find('ul',{'class':'popularCities'})).children\n",
    "        print('Great Mega City present')\n",
    "    except:\n",
    "        print('I think your mega city is not present')\n",
    "        print('trying one more time')\n",
    "        try:\n",
    "            cities = (soup.find('ul',{'class':'otherCities'})).children\n",
    "            print('Hurray!!! City Present')\n",
    "        except:\n",
    "            print('Oops!! city is not present')\n",
    "            print('Try any big city near you')\n",
    "    return soup,cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ee7c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def click(driver):\n",
    "    try:\n",
    "        soup,cities = get_city(driver)\n",
    "        driver.find_element(By.CSS_SELECTOR,\"img[data-title='{}']\".format(next(cities).get('data-title'))).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        soup,cities = get_city(driver)\n",
    "        print('first pass failed!!! trying second pass')\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,\"//li[text()='{}']\".format(next(cities).text)).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        soup,cities = get_city(driver)\n",
    "        print('second pass failed!!! trying second pass')\n",
    "    try:\n",
    "        driver.find_element(By.LINK_TEXT,\"{}\".format(next(cities).text)).click()\n",
    "        print('Done')\n",
    "        return\n",
    "    except:\n",
    "        print('Oops..... no city is present to click')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8dc42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(soup):\n",
    "    title = \"As per cars24 details : \"+str(soup.find('div',{'class':'js-content'}).find(\"h1\").text)\n",
    "    print(title)\n",
    "    print('NOTE: total cars on website can be different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "123155fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(cars):\n",
    "    all_cars = []\n",
    "    print(\"observed entries: \",len(cars))\n",
    "    for i,j in enumerate(tqdm(cars,desc = \"Collecing data :...\")):\n",
    "        try:\n",
    "            title = cars[i].find('h2',{'class':'_3FpCg'}).text\n",
    "            model = (\" \").join(cars[i].find('p',{'class':'cvakB'}).text.split(' ')[:-1])\n",
    "            gear_type = cars[i].find('p',{'class':'cvakB'}).text.split(' ')[-1]\n",
    "            km = cars[i].find_all('li')[0].text\n",
    "            owner = cars[i].find_all('li')[1].text\n",
    "            fuel = cars[i].find_all('li')[2].text\n",
    "            state_passing = cars[i].find_all('li')[3].text\n",
    "            price = cars[i].find('div',{'class':'_7udZZ'}).text.replace(\"₹\",\"\")\n",
    "            all_cars.append((title,model,gear_type,km,owner,fuel,state_passing,price))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"total cars collected \",len(all_cars))\n",
    "    print(\"Saving in CSV file\")\n",
    "    filename='cars data-'+str(city_name)+str('.csv')\n",
    "    with open(filename,'w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Car_Name','version','Gear Type','Running_kn','Owner','Fuel Type','Passing State','Price'])\n",
    "        writer.writerows(all_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9e3b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please share city name: Mumbai\n",
      "Browser Started...\n",
      "Searching cars24.com...\n",
      "https://www.cars24.com/buy-used-car?sort=P&storeCityId=2&pinId=110001\n",
      "....\n",
      "Search box is clear.....\n",
      "Great Mega City present\n",
      "Done\n",
      "As per cars24 details : 580 Used Cars in Mumbai\n",
      "NOTE: total cars on website can be different\n",
      "scrolling Down\n",
      "current_count :21 21\n",
      "scrolling Down\n",
      "current_count :333 333\n",
      "scrolling Down\n",
      "current_count :553 553\n",
      "scrolling Down\n",
      "current_count :581 581\n",
      "reached to end\n",
      "observed entries:  581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecing data :...: 100%|█████████████████████████████████████████████████████████| 581/581 [00:00<00:00, 1600.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cars collected  580\n",
      "Saving in CSV file\n",
      "Browser Closed\n"
     ]
    }
   ],
   "source": [
    "city_name = str(input('Please share city name: '))\n",
    "driver = start_browser()\n",
    "driver.get(get_url())\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[1]/div[2]/p\").click()\n",
    "a = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div[1]/div/div[2]/div/input\")\n",
    "try:\n",
    "    driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div[1]/div/div[2]/img\").click()\n",
    "except:\n",
    "    print('Search box is clear.....')\n",
    "a.send_keys(city_name)\n",
    "click(driver)\n",
    "time.sleep(5)\n",
    "print_title(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "\n",
    "l = 0\n",
    "while(True):\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    cars = soup.find_all('div',{'class':'col-4'})\n",
    "    if l == len(cars):\n",
    "        print(\"reached to end\")\n",
    "        break\n",
    "    else:\n",
    "        l = len(cars)\n",
    "        print('scrolling Down')\n",
    "        print('current_count :'+str(len(cars))+\" \"+str(l))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(8)\n",
    "\n",
    "save_file(cars)\n",
    "driver.quit()\n",
    "print('Browser Closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0f2c5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Mega City present\n",
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9d0d90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "83ccd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "de90434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Found \"+str(soup.find('div',{'class':'js-content'}).find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "276d1766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Found 60 Used Cars in Nashik'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "52f342e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrolling Down\n",
      "current_count :21 21\n",
      "scrolling Down\n",
      "current_count :193 193\n",
      "scrolling Down\n",
      "current_count :373 373\n",
      "scrolling Down\n",
      "current_count :513 513\n",
      "scrolling Down\n",
      "current_count :582 582\n",
      "reached to end\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "while(True):\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    cars = soup.find_all('div',{'class':'col-4'})\n",
    "    if l == len(cars):\n",
    "        print(\"reached to end\")\n",
    "        break\n",
    "    else:\n",
    "        l = len(cars)\n",
    "        print('scrolling Down')\n",
    "        print('current_count :'+str(len(cars))+\" \"+str(l))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f5c083b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed entries:  582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 582/582 [00:00<00:00, 2209.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cars collected  581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb88a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16,14,000'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars[0].find('div',{'class':'_7udZZ'}).text.replace(\"₹\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "067a8ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7 ZX AT 7 STR Automatic'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars[0].find('p',{'class':'cvakB'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f4058c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7 ZX AT 7 STR'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\" \").join(cars[0].find('p',{'class':'cvakB'}).text.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958ed9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c4236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
